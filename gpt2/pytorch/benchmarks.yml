---
train_options: &train_options
  location: public_examples/nlp/gpt2/pytorch/
  data:
    throughput:
      reduction_type: "final"
      regexp: 'Throughput: ([0-9, .]*)'
    loss:
      reduction_type: "final"
      regexp: "loss: *(.*?),"
    accuracy:
      reduction_type: "final"
      regexp: "acc: *(.*?),"
  output:
    - [samples/sec, "throughput"]
    - [Loss, "loss"]
    - [Accuracy, "accuracy"]

multihost_options: &multihost_options
  description: |
    GPT2 training for 64/128/256 IPUs.
    The env variables needed are:
      HOSTS - Comma separated list of hosts, usually something like:
      `10.1.3.101,10.1.3.10x...`
      VIPU_SERVER_HOST - IP of host where VIPU server is running. Usually
      `10.1.3.101`.
      PARTITION - Name of partition to use, can be found via
      `vipu list partitions`. Note that the `--update-partitions` poprun
      flag is disabled here, so ensure the partition is reconfigurable
      or it has been setup properly for this benchmark
      TCP_IF_INCLUDE - Name of subnet, or range of IPs for network
      interface on which all hosts are. Usually `enp65s0f0np0` or
      `10.1.3.0/24`.

pytorch_gpt2_small_16_ipu_training_generated:
  <<: *train_options
  description: |
    GPT2 Small training for 16 IPUs with generated data.
  cmd: >-
    python train_gpt2.py
      --model gpt2
      --ipus-per-replica 4
      --replication-factor 4
      --gradient-accumulation 2048
      --batches-per-step 8
      --batch-size 1
      --layers-per-ipu 0 4 4 4
      --matmul-proportion 0.15 0.15 0.15 0.15
      --max-len 1024
      --optimizer AdamW
      --learning-rate 0.00015
      --lr-schedule cosine
      --lr-warmup 0.01
      --remap-logit True
      --enable-sequence-serialized True
      --embedding-serialization-factor 4
      --recompute-checkpoint-every-layer True
      --enable-half-partials True
      --replicated-tensor-sharding True
      --compile-only False
      --train-path 'generated'
      --epochs 1

pytorch_gpt2_small_64_ipu_training_generated:
  <<: *train_options
  description: |
    GPT2 Small training for 64 IPUs with generated data.
  cmd: >-
    python train_gpt2.py
      --model gpt2
      --ipus-per-replica 4
      --replication-factor 16
      --gradient-accumulation 512
      --batches-per-step 8
      --batch-size 1
      --layers-per-ipu 0 4 4 4
      --matmul-proportion 0.15 0.15 0.15 0.15
      --max-len 1024
      --optimizer AdamW
      --learning-rate 0.00015
      --lr-schedule cosine
      --lr-warmup 0.01
      --remap-logit True
      --enable-sequence-serialized True
      --embedding-serialization-factor 4
      --recompute-checkpoint-every-layer True
      --enable-half-partials True
      --replicated-tensor-sharding True
      --compile-only False
      --train-path 'generated'
      --epochs 1

pytorch_gpt2_medium_16_ipu_training_generated:
  <<: *train_options
  description: |
    GPT2 Medium training for 16 IPUs with generated data.
  cmd: >-
    python train_gpt2.py
      --model gpt2-medium
      --ipus-per-replica 8
      --replication-factor 2
      --gradient-accumulation 4096
      --device-iterations 8
      --batch-size 1
      --layers-per-ipu 0 3 3 3 3 4 4 4
      --matmul-proportion 0.30 0.15 0.15 0.15 0.15 0.15 0.15 0.15
      --max-len 1024
      --optimizer AdamW
      --learning-rate 0.00015
      --lr-schedule cosine
      --lr-warmup 0.01
      --remap-logit True
      --enable-sequence-serialized True
      --embedding-serialization-factor 4
      --recompute-checkpoint-every-layer True
      --enable-half-partials True
      --replicated-tensor-sharding True
      --compile-only False
      --train-path 'generated'
      --epochs 1

pytorch_gpt2_medium_64_ipu_training_generated:
  <<: *train_options
  description: |
    GPT2 Medium training for 64 IPUs with generated data.
  cmd: >-
    python train_gpt2.py
      --model gpt2-medium
      --ipus-per-replica 8
      --replication-factor 8
      --gradient-accumulation 1024
      --device-iterations 8
      --batch-size 1
      --layers-per-ipu 0 3 3 3 3 4 4 4
      --matmul-proportion 0.30 0.15 0.15 0.15 0.15 0.15 0.15 0.15
      --max-len 1024
      --optimizer AdamW
      --learning-rate 0.00015
      --lr-schedule cosine
      --lr-warmup 0.01
      --remap-logit True
      --enable-sequence-serialized True
      --embedding-serialization-factor 4
      --recompute-checkpoint-every-layer True
      --enable-half-partials True
      --replicated-tensor-sharding True
      --compile-only False
      --train-path 'generated'
      --epochs 1

pytorch_gpt2_medium_256_ipu_training_generated:
  <<: *train_options
  description: |
    GPT2 Medium training for 256 IPUs with generated data.
  cmd: >-
    poprun
      -vv
      --host $HOSTS
      --num-instances=4
      --num-replicas=32
      --num-ilds=4
      --ipus-per-replica=8
      --vipu-server-host=$VIPU_SERVER
      --vipu-partition=$PARTITON
      --reset-partition=no
      --vipu-server-timeout=3600
      --mpi-global-args="
        --mca oob_tcp_if_include $TCP_IF_INCLUDE
        --mca btl_tcp_if_include $TCP_IF_INCLUDE"
      --mpi-local-args="
        -x OPAL_PREFIX
        -x LD_LIBRARY_PATH
        -x PATH
        -x PYTHONPATH
        -x CPATH
        -x IPUOF_VIPU_API_TIMEOUT=3600
        -x POPLAR_LOG_LEVEL=WARN
        -x POPLAR_SDK_ENABLED
        -x POPLAR_ENGINE_OPTIONS"
    python train_gpt2.py
      --model gpt2-medium
      --ipus-per-replica 8
      --replication-factor 32
      --gradient-accumulation 256
      --device-iterations 8
      --batch-size 1
      --layers-per-ipu 0 3 3 3 3 4 4 4
      --matmul-proportion 0.30 0.15 0.15 0.15 0.15 0.15 0.15 0.15
      --max-len 1024
      --optimizer AdamW
      --learning-rate 0.00015
      --lr-schedule cosine
      --lr-warmup 0.01
      --remap-logit True
      --enable-sequence-serialized True
      --embedding-serialization-factor 4
      --recompute-checkpoint-every-layer True
      --enable-half-partials True
      --replicated-tensor-sharding True
      --compile-only False
      --train-path 'generated'
      --epochs 1

pytorch_gpt2_large_sl512_16_ipu_training_generated:
  <<: *train_options
  description: |
    GPT2 Large(length=512) training for 16 IPUs with generated data.
  cmd: >-
    python train_gpt2.py
      --model gpt2-large
      --ipus-per-replica 8
      --replication-factor 2
      --gradient-accumulation 4096
      --device-iterations 8
      --batch-size 1
      --layers-per-ipu 1 5 5 5 5 5 5 5
      --matmul-proportion 0.15 0.12 0.15 0.15 0.15 0.15 0.15 0.15
      --max-len 512
      --optimizer AdamW
      --learning-rate 0.00015
      --lr-schedule cosine
      --lr-warmup 0.01
      --remap-logit True
      --enable-sequence-serialized True
      --embedding-serialization-factor 8
      --recompute-checkpoint-every-layer True
      --enable-half-partials True
      --compile-only False
      --replicated-tensor-sharding True
      --train-path 'generated'
      --epochs 1

pytorch_gpt2_large_sl512_64_ipu_training_generated:
  <<: *train_options
  description: |
    GPT2 Large(length=512) training for 64 IPUs with generated data.
  cmd: >-
    python train_gpt2.py
      --model gpt2-large
      --ipus-per-replica 8
      --replication-factor 8
      --gradient-accumulation 1024
      --device-iterations 8
      --batch-size 1
      --layers-per-ipu 1 5 5 5 5 5 5 5
      --matmul-proportion 0.15 0.12 0.15 0.15 0.15 0.15 0.15 0.15
      --max-len 512
      --optimizer AdamW
      --learning-rate 0.00015
      --lr-schedule cosine
      --lr-warmup 0.01
      --remap-logit True
      --enable-sequence-serialized True
      --embedding-serialization-factor 8
      --recompute-checkpoint-every-layer True
      --enable-half-partials True
      --compile-only False
      --replicated-tensor-sharding True
      --train-path 'generated'
      --epochs 1

pytorch_gpt2_large_sl512_256_ipu_training_generated:
  <<: *train_options
  description: |
    GPT2 Large(length=512) training for 256 IPUs with generated data.
  cmd: >-
    poprun
      -vv
      --host $HOSTS
      --num-instances=4
      --num-replicas=32
      --num-ilds=4
      --ipus-per-replica=8
      --vipu-server-host=$VIPU_SERVER
      --vipu-partition=$PARTITON
      --reset-partition=no
      --vipu-server-timeout=3600
      --mpi-global-args="
        --mca oob_tcp_if_include $TCP_IF_INCLUDE
        --mca btl_tcp_if_include $TCP_IF_INCLUDE"
      --mpi-local-args="
        -x OPAL_PREFIX
        -x LD_LIBRARY_PATH
        -x PATH
        -x PYTHONPATH
        -x CPATH
        -x IPUOF_VIPU_API_TIMEOUT=3600
        -x POPLAR_LOG_LEVEL=WARN
        -x POPLAR_SDK_ENABLED
        -x POPLAR_ENGINE_OPTIONS"
    python train_gpt2.py
      --model gpt2-large
      --ipus-per-replica 8
      --replication-factor 32
      --gradient-accumulation 256
      --device-iterations 8
      --batch-size 1
      --layers-per-ipu 1 5 5 5 5 5 5 5
      --matmul-proportion 0.15 0.12 0.15 0.15 0.15 0.15 0.15 0.15
      --max-len 512
      --optimizer AdamW
      --learning-rate 0.00015
      --lr-schedule cosine
      --lr-warmup 0.01
      --remap-logit True
      --enable-sequence-serialized True
      --embedding-serialization-factor 8
      --recompute-checkpoint-every-layer True
      --enable-half-partials True
      --compile-only False
      --replicated-tensor-sharding True
      --train-path 'generated'
      --epochs 1

pytorch_gpt2_large_16_ipu_training_generated:
  <<: *train_options
  description: |
    GPT2 Large(length=1024) training for 16 IPUs with generated data.
  cmd: >-
    python train_gpt2.py
      --model gpt2-large
      --ipus-per-replica 16
      --replication-factor 1
      --gradient-accumulation 8192
      --batches-per-step 8
      --batch-size 1
      --layers-per-ipu 0 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2
      --matmul-proportion 0.2 0.15 0.2 0.2 0.2 0.15 0.15 0.2 0.2 0.15 0.2 0.2 0.2 0.15 0.15 0.2
      --max-len 1024
      --optimizer AdamW
      --learning-rate 0.00015
      --lr-schedule cosine
      --lr-warmup 0.01
      --remap-logit True
      --enable-sequence-serialized True
      --embedding-serialization-factor 4
      --recompute-checkpoint-every-layer True
      --enable-half-partials True
      --compile-only False
      --replicated-tensor-sharding False
      --train-path 'generated'
      --epochs 1

inference_options: &inference_options
  location: public_examples/nlp/gpt2/pytorch/
  data:
    throughput:
      reduction_type: "final"
      regexp: 'Throughput: ([0-9, .]*)'
    latency:
      reduction_type: "final"
      regexp: 'Latency: ([0-9, .]*)'
  output:
    - [samples/sec, "throughput"]
    - [sec, "latency"]

pytorch_gpt2_xlarge_16_ipu_inference_generated:
  <<: *inference_options
  description: |
    GPT2 XLarge inference for 16 IPUs with generated data.
  cmd: >-
    python inference_gpt2.py
      --model gpt2-xl
      --max-len 1024
      --layers-per-ipu 1 6 6 7 7 7 7 7
      --matmul-proportion 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2
      --ipus-per-replica 8
      --replication-factor 2
      --epochs 2
      --batches-per-step 1
      --batch-size 1
      --gradient-accumulation 1
      --embedding-serialization-factor 4
      --enable-half-partials True
      --train-path 'generated'
      --replicated-tensor-sharding False
      --compile-only False

generation_options: &generation_options
  location: public_examples/nlp/gpt2/pytorch/
  data:
    throughput:
      reduction_type: "final"
      regexp: 'Throughput: ([0-9, .]*)'
    latency:
      reduction_type: "final"
      regexp: 'Latency: *(.*?) sec/token'
  output:
    - [samples/sec, "throughput"]
    - [sec/token, "latency"]

pytorch_gpt2_medium_4_ipu_generation:
  <<: *generation_options
  description: |
    GPT2 Medium text generation for 4 IPUs with real data.
  cmd: >-
    python text_generate_gpt2.py
      --model-name-or-path gpt2-medium
      --prompt Hello
      --fp16 true
      --single-ipu false
      --poptorch-loop false
      --layers-per-ipu 1 7 8 8
      --matmul-proportion 0.2 0.2 0.2 0.2
      --output-len 256
